
import sqlite3
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_groq import ChatGroq
from langchain.schema import HumanMessage
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any, Literal
import utils.DataModels as dm
from langchain.output_parsers import PydanticOutputParser
from langchain.schema.messages import HumanMessage
from utils.plotting import plot_chart
import re

# Create parser for your LLMResponse model
parser = PydanticOutputParser(pydantic_object=dm.LLMResponse)

def get_llm(provider, api_key):
    if provider == "openai":
        return ChatOpenAI(model="gpt-4.1-nano", api_key=api_key)
    elif provider == "groq":
        return ChatGroq(model="llama3-8b-8192", api_key=api_key)

def get_db_schema_and_sample(conn, table_name="customer_data"):
    cursor = conn.cursor()
    cursor.execute(f"PRAGMA table_info({table_name})")
    schema_info = cursor.fetchall()
    columns = [(col[1], col[2]) for col in schema_info]  # (column_name, data_type)

    df_sample = pd.read_sql_query(f"SELECT * FROM {table_name} LIMIT 5", conn)
    return columns, df_sample

def generate_prompt(user_question, schema):
    schema_str = "\n".join(f"- {name}: {dtype}" for name, dtype in schema)
    return 


# def execute_code(code: str, df_result: pd.DataFrame):
#     """
#     Execute python code string in a minimal environment with df_result, plt, pd, np available.
#     Returns matplotlib Figure if created, or resulting DataFrame if table_code.
#     """
#     local_vars = {
#         'df_result': df_result,
#         'plt': plt,
#         'pd': pd,
#         'np': np,
#         'fig': None,
#         'result_table': None
#     }

#     plt.close('all')  # clear any previous figures

#     try:
#         exec(code, {}, local_vars)
#         fig = local_vars.get('fig') or plt.gcf()
#         result_table = local_vars.get('result_table')
#         return fig, result_table
#     except Exception as e:
#         print(f"Code execution error: {e}")
#         return None, None


def generate_structured_sql(llm, question, columns, df_sample, table_name="customer_data"):
    context = build_prompt_context(question, columns, df_sample, table_name)

    prompt = f"""
{context}

You are an AI that generates SQLite queries.

Return only the raw SQL query (no markdown, no explanation).
"""
    response = llm.invoke([HumanMessage(content=prompt)])
    sql_query = response.content.strip()
    
    return dm.SQLQuery(sql=sql_query, explanation="Generated by LLM")


def extract_sql(text):
    # Case 1: Pure SQL query with no extra text → return directly
    if text.strip().lower().startswith("select") or " from " in text.lower():
        return text.strip()
    
    # Case 2: Markdown-formatted SQL block
    match = re.search(r"```sql(.*?)```", text, re.DOTALL | re.IGNORECASE)
    if match:
        return match.group(1).strip()

    # Case 3: Strip narration like "Here is the query:"
    lines = text.strip().splitlines()
    clean_lines = [
        line for line in lines
        if not line.strip().lower().startswith("here") and not line.strip().startswith("```")
    ]
    return "\n".join(clean_lines).strip()


def execute_sql_query(conn, sql_query):
    try:
        sql_query = sql_query.strip()
        sql_query=extract_sql(sql_query)
        df_result = pd.read_sql_query(sql_query, conn)
        return df_result, None
    except Exception as e:
        error_msg = f"SQL Error: {str(e)}\nGenerated SQL: {sql_query}"
        return None, error_msg


# def llm_needs_sql(llm, question, columns, df_sample, table_name="customer_data"):
#     context = build_prompt_context(question, columns, df_sample, table_name)

#     prompt = f"""
# {context}

# You are deciding whether a user's question requires a SQL query.

# If SQL is NOT needed, answer the user's question directly based on the schema. If SQL IS needed, just say "yes".
# If the user's question involves calculations, aggregations, grouping, filtering, or any kind of summary/statistical analysis SQL is required.

# Examples:
# Q: "What is the total revenue per category?" → yes  
# Q: "Summarize the table" → yes  
# Q: "What is a DataFrame?" → no | "A DataFrame is a tabular data structure..."  
# Q: "List last few things I asked" → no  
# Q: "What is the data about?" → no | "The data contains customer bank activity and churn information."  
# Q: "What is the SQL syntax for inner join?" → no
# Q: "Show me the average age and credit score by country and gender." → yes  
# Q: "Provide a chart of churn rate by country." → yes  
# Q: "Plot the distribution of account balances." → yes 
# Q: "provide me the count of (any column OR rows)" → yes 

# Now answer for:
# Q: "{question}"

# Reply with "yes" if SQL is needed.  
# If not, reply with "no | <direct answer here>".
# """

#     result = llm.invoke([HumanMessage(content=prompt)])
#     output = result.content.strip()
#     if output.lower().startswith("yes"):
#         return True, ""
#     elif output.lower().startswith("no"):
#         # Extract the answer if present after the pipe symbol
#         parts = output.split('|', 1)
#         answer = parts[1].strip() if len(parts) > 1 else ""
#         return False, answer
#     else:
#         # Fallback: assume SQL needed if unclear
#         return True, ""

def llm_needs_sql(llm, question, columns, df_sample, table_name="customer_data"):
    context = build_prompt_context(question, columns, df_sample, table_name)

# ## Sample Questions:
# Q: "What is the total revenue per category?" → yes  
# Q: "Summarize the table" → yes  
# Q: "What is a DataFrame?" → no | A DataFrame is a tabular data structure with labeled rows and columns.  
# Q: "What does the dataset contain?" → no | The table contains customer data related to bank activity and churn.
# Q: "Show me average age by gender and country" → yes  
# Q: "What columns are available?" → no | The table contains: {', '.join(columns)}  
# Q: "How is 'churn' defined in the table?" → no | Based on column name, churn indicates whether a customer left.  
# Q: "Plot a histogram of balance column" → yes  
# Q: "Count how many customers are active" → yes

    prompt = f"""
{context}

You are an AI data analyst. Your job is to determine whether answering the user's question requires executing a SQL query on the dataset.

## All the questions are related to this database, the schema of which is provided above.

## Decision Rules:
- SQL **is NOT required** ONLY if:
   - The user is asking about the schema or structure of the table itself, such as:
     - "What columns does the table have?"
     - "What is the purpose of the 'churn' column?"
     - "What types of values does 'country' contain?"
     - "What does 'credit_score' represent?"

- SQL **is required** if the question involves:
  - Aggregations (sum, avg, min, max, count)
  - Filtering rows (e.g. by country, gender, score)
  - ANY form of data lookup or numeric result is required to answer the question (e.g., totals, counts, averages, comparisons, even if it seems simple)
  - Grouping by categories
  - - Any sort of statistics or data manipulation, such as:
    - "What is the average balance of customers over 40?"
    - "How many customers hold more than 2 products?"
    - "Show the distribution of credit scores"
    - "Compare average tenure between churned and non-churned customers"
    - "Which gender has the highest average salary?"
    - "What percentage of customers are active members?"
    - "Give the standard deviation of estimated salary per country"
    - "What is the churn rate for each age group?"
  - Creating a chart from filtered/aggregated data

## Decision Instructions (You MUST follow):
- You MUST reply `yes` if the answer requires accessing the actual data values, even for a simple count or filter.
- You MUST reply `no | <short explanation>` only if the question is purely about schema, column names, or metadata.
- If you are unsure, default to `yes`.

## Examples:
Q: "What is the total revenue per category?" → yes  
Q: "Summarize the table" → yes  
Q: "What is a DataFrame?" → no | A DataFrame is a tabular data structure...
Q: "List last few things I asked" → no  
Q: "What is the data about?" → no | The data contains customer bank activity and churn information.  
Q: "What is the SQL syntax for inner join?" → no
Q: "Show me the average age and credit score by country and gender." → yes  
Q: "Provide a chart of churn rate by country." → yes  
Q: "Plot the distribution of account balances." → yes  
Q: "How many male customers are there?" → yes  
Q: "Which country has the highest average salary?" → yes  
Q: "Give the standard deviation of age per gender." → yes  
Q: "Which gender has higher churn?" → yes  
Q: "What columns are in the data?" → no | The dataset contains columns like age, country, churn, etc.  
Q: "What does the churn column represent?" → no | It shows whether the customer left the bank (1) or stayed (0).

## Now answer this:
Q: "{question}"
"""

    result = llm.invoke([HumanMessage(content=prompt)])
    output = result.content.strip()
    if output.lower().startswith("yes"):
        return True, ""
    elif output.lower().startswith("no"):
        # Extract the answer if present after the pipe symbol
        parts = output.split('|', 1)
        answer = parts[1].strip() if len(parts) > 1 else ""
        return False, answer
    else:
        # Fallback: assume SQL needed if unclear
        return True, ""


def build_prompt_context(question, columns, df_sample, table_name="customer_data"):
    schema_str = "\n".join([f"{name}: {dtype}" for name, dtype in columns])

    # Detailed column descriptions for accurate SQL generation
    column_details = """
Column Details:
- customer_id: Unique integer identifier for each customer (Primary Key, e.g., 15634602). Not used for analytics, mainly for identification.
- credit_score: Customer's credit score (integer, typically 300-850). Indicates creditworthiness.
- country: Country where the customer resides (categorical: France, Spain, Germany). Useful for regional segmentation.
- gender: Customer's gender (categorical: Male, Female).
- age: Customer's age in years (integer, e.g., 42). Numeric, suitable for range queries, aggregation, and segmentation.
- tenure: Number of years the customer has been with the bank (integer, 0-10). Useful for loyalty/retention analytics.
- balance: Account balance (float, can be 0, e.g., 159660.8). Represents the money in the customer's account.
- products_number: Number of bank products held by the customer (integer, e.g., 1-4). Useful for understanding customer engagement.
- credit_card: Whether the customer has a credit card (binary: 1 = Yes, 0 = No).
- active_member: Whether the customer is an active member (binary: 1 = Yes, 0 = No).
- estimated_salary: Estimated annual salary of the customer (float, e.g., 101348.88). Useful for income-based segmentation.
- churn: Target column. Indicates if the customer has left the bank (1 = Yes, 0 = No). Use this for churn prediction, not as a filter for retained customers unless explicitly asked.

Notes:
- Some numeric fields may be stored as TEXT. Use `CAST(column AS INTEGER/REAL)` as needed in SQL.
"""

    return f"""
You are working with a SQLite table.

Table name: {table_name}

Schema:
{schema_str}

{column_details}

User question: "{question}"
"""

def build_prompt(question: str, df_markdown: str, columns: list, parser) -> str:
    allowed_charts = ['bar', 'pie', 'line', 'scatter']
    format_instructions = parser.get_format_instructions()
    column_str = ", ".join(columns)

    return f"""
You are a data analyst. A user has asked a question, and the data shown below has already been **filtered or aggregated appropriately using a SQL query** based on that question.

This means:
- The data you're seeing is the **direct answer** to the user's question.
- Do not assume or speculate about what data is missing.
- Do not explain how to calculate it — it has already been calculated.

Your task is to provide:
1. A clear and concise explanation of the result in the 'text' field.
2. Optionally, a chart specification in the 'chart' field **if it helps visualize the result better**.

If a chart will make the answer clearer, set the 'chart' field to a JSON object with:
- chart_type: one of {allowed_charts}
- x_column: (column name for x-axis, or null)
- y_column: (column name for y-axis, or null)
- groupby_column: (column to group by if needed, or null)
- aggregation: (mean, sum, count, etc, or null)
- reason: (why this chart helps the answer)

If no chart is needed, set 'chart' to null.

---

Data:
{df_markdown}

User Question:
{question}

Columns in the data are: {column_str}

{format_instructions}
"""

# def plot_chart(df, chart_metadata: ChartMetadata):
#     if not chart_metadata or not chart_metadata.chart_type:
#         return None  # No chart requested

#     chart_type = chart_metadata.chart_type
#     x = chart_metadata.x_column
#     y = chart_metadata.y_column

#     fig, ax = plt.subplots()
#     if chart_type == 'pie':
#         # if x and y:
#         #     # x should be labels, y should be values (already aggregated)
#         #     data = df.set_index(x)[y]
#         #     data.plot.pie(autopct='%1.1f%%', ax=ax)
#         #     ax.set_ylabel('')
#         #     ax.set_title(f"{y.replace('_', ' ').title()} by {x.replace('_', ' ').title()}")
#         # else:
#         #     raise ValueError("Pie chart needs x_column and y_column.")
#             # Handle fallback for pie chart: infer x and y if missing
#     # if chart_type == 'pie' and (not x or not y):
#         if chart_metadata.groupby_column and chart_metadata.aggregation:
#             groupby = chart_metadata.groupby_column
#             agg = chart_metadata.aggregation
#             if groupby in df.columns:
#                 # Aggregate the data and update x and y
#                 df = df.groupby(groupby).agg({df.columns[0]: agg}).reset_index()
#                 x = groupby
#                 y = df.columns[1]  # the aggregated column

#     elif chart_type == 'bar':
#         if x and y:
#             df.plot(x=x, y=y, kind='bar', ax=ax, legend=False)
#             ax.set_xlabel(x.replace('_', ' ').title())
#             ax.set_ylabel(y.replace('_', ' ').title())
#             ax.set_title(f"{y.replace('_', ' ').title()} by {x.replace('_', ' ').title()}")
#             ax.set_xticklabels(df[x], rotation=0)
#         else:
#             raise ValueError("Bar chart needs x_column and y_column.")
#     elif chart_type == 'line':
#         if x and y:
#             df.plot(x=x, y=y, kind='line', ax=ax, legend=False)
#             ax.set_xlabel(x.replace('_', ' ').title())
#             ax.set_ylabel(y.replace('_', ' ').title())
#             ax.set_title(f"{y.replace('_', ' ').title()} by {x.replace('_', ' ').title()}")
#         else:
#             raise ValueError("Line chart needs x_column and y_column.")
#     elif chart_type == 'scatter':
#         if x and y:
#             df.plot.scatter(x=x, y=y, ax=ax)
#             ax.set_xlabel(x.replace('_', ' ').title())
#             ax.set_ylabel(y.replace('_', ' ').title())
#             ax.set_title(f"{y.replace('_', ' ').title()} vs {x.replace('_', ' ').title()}")
#         else:
#             raise ValueError("Scatter plot needs x_column and y_column.")
#     else:
#         raise ValueError(f"Unsupported chart type: {chart_type}")

#     plt.tight_layout()
#     return fig



def analyze_data_with_llm(llm, question, df_result, parser, columns):
    df_markdown = df_result.to_markdown(index=False)
    prompt = build_prompt(question, df_markdown, columns, parser)

    response = llm.invoke([HumanMessage(content=prompt)])
    print(response)
    parsed = parser.parse(response.content)
    return parsed


#main caller function
def run_llm_data_flow(conn, question, llm, table_name="customer_data", parser=parser):

    # Step 1: Get database schema
    columns, df_sample = get_db_schema_and_sample(conn, table_name=table_name)
    print("fetching schema sucessful")

    # needs_sql = llm_needs_sql(llm, question, columns, df_sample, table_name)
    needs_sql, answer = llm_needs_sql(llm, question, columns, df_sample, table_name)
    if not needs_sql:
        print("No sql needed")
        response_dict = {"text": answer}
        dummy_df = None
        return dummy_df, response_dict
    print("SQL needed")
    
    # Step 2: Generate SQL query using LLM
    sql_query_obj = generate_structured_sql(llm, question, columns, df_sample, table_name=table_name)
    print(f"generated SQL query\n{sql_query_obj.sql}")
    # Step 3: Execute SQL to get data
    df_result, error = execute_sql_query(conn, sql_query_obj.sql)
    if error:
        print(f"error occured,\n{error} ")
        return None, {"type": "error", "error": error}

    # Step 4: Send data + user question to LLM for final analysis
    column_names = [c[0] for c in columns]
    final_result = analyze_data_with_llm(llm, question, df_result, parser, column_names)
    response_dict = {"text": final_result.text}
    print(final_result.text)
    if final_result.chart:
        print(final_result.chart)
        fig = plot_chart(df_result, final_result.chart)
        if fig:
            response_dict["plot_figure"] = fig

    return df_result, response_dict
